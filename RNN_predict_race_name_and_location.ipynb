{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c3e935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation, LSTM\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c80810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df,sample,ntest,ngrams,feature_len,model,strat_sampling=False,random_state=21):\n",
    "    \n",
    "    #Cleaning:\n",
    "    df.dropna(subset=['name_first', 'name_last'], inplace=True)\n",
    "    \n",
    "    #Capitalize the first letters:\n",
    "    df['name_first'] = df.name_first.str.title()\n",
    "    df['name_last'] = df.name_last.str.title()\n",
    "    \n",
    "    # Concatenate the data depending on the model:\n",
    "    if model=='fl_fips':\n",
    "        df['name_last_name_first'] = df['name_last'] + ' ' + df['name_first']+' '+df['fips']\n",
    "    elif model=='fl_zip':\n",
    "        df['name_last_name_first'] = df['name_last'] + ' ' + df['name_first']+' '+df['zip']\n",
    "    elif model=='l':\n",
    "        df['name_last_name_first'] = df['name_last']\n",
    "    elif model=='fl' or model=='fl_zipcomp':\n",
    "        df['name_last_name_first'] = df['name_last'] + ' ' + df['name_first']\n",
    "    else:\n",
    "        sys.exit('Invalid model name. Should take one of the following values: fl_fips, fl_zip, fl_zipcomp, fl, l')\n",
    "        \n",
    "    #Randomly shuffle the data:    \n",
    "    df=shuffle(df, random_state=random_state)\n",
    "    \n",
    "    #Split the data into test, dev and train sets:\n",
    "    ndev=2*ntest\n",
    "    \n",
    "    sdf_test=df[:ntest]\n",
    "    sdf_dev=df[ntest:ndev]\n",
    "    sdf_train=df[ndev:]\n",
    "    \n",
    "    #Sample train sample:\n",
    "    if strat_sampling==True:\n",
    "    \n",
    "        sample_non_white=sdf_train[sdf_train['race']!=\"nh_white\"].groupby('race', group_keys=False).apply(lambda x: x.sample(min(len(x),sample),random_state=21))\n",
    "        sample_white=sdf_train[sdf_train['race']==\"nh_white\"].sample(n=sample, random_state=random_state)\n",
    "\n",
    "        sdf_train=pd.concat([sample_non_white,sample_white])\n",
    "    \n",
    "        sdf_train=shuffle(sdf_train, random_state=random_state)\n",
    "        \n",
    "    else:\n",
    "        sdf_train=sdf_train.sample(n=sample, random_state=random_state)\n",
    "    \n",
    "    #Print the breakdown of number of observations by race in the train sample:\n",
    "    \n",
    "    print(sdf_train.groupby('race').agg({'name_first': 'count'}))\n",
    "    \n",
    "    \n",
    "    #Concat test,dev and train samples:\n",
    "    \n",
    "    sdf=pd.concat([sdf_test,sdf_dev,sdf_train])\n",
    "\n",
    "    #Build n-gram list:\n",
    "    vect = CountVectorizer(analyzer='char', max_df=0.3, min_df=3, ngram_range=ngrams, lowercase=False) \n",
    "    a = vect.fit_transform(sdf.name_last_name_first)\n",
    "    b = vect.inverse_transform(a)\n",
    "    vocab = vect.vocabulary_\n",
    "    \n",
    "    #Number of words in the vocabulary:\n",
    "    print(\"Number of words:\",len(vocab))\n",
    "    \n",
    "    #Implement one-hot encoding:\n",
    "    X=[]\n",
    "    for i in b:\n",
    "        x=[]\n",
    "        for j in i:\n",
    "            x.append(vocab[j])\n",
    "        X.append(x)\n",
    "\n",
    "    # Check max/avg feature\n",
    "    X_len = []\n",
    "    for x in X:\n",
    "        X_len.append(len(x))\n",
    "\n",
    "    max_feature_len = max(X_len)\n",
    "    avg_feature_len = int(np.mean(X_len))\n",
    "\n",
    "    print(\"Max feature len = %d, Avg. feature len = %d\" % (max_feature_len, avg_feature_len))\n",
    "    y = np.array(sdf.race.astype('category').cat.codes)\n",
    "\n",
    "    # Split train, dev and test datasets\n",
    "    \n",
    "    X_test=X[:ntest]\n",
    "    y_test=y[:ntest]\n",
    "    \n",
    "    X_dev=X[ntest:ndev]\n",
    "    y_dev=y[ntest:ndev]\n",
    "    \n",
    "    X_train=X[ndev:]\n",
    "    y_train=y[ndev:]\n",
    "\n",
    "    print(len(X_train), 'train sequences')\n",
    "    print(len(X_dev), 'dev sequences')\n",
    "    print(len(X_test), 'test sequences')\n",
    "    \n",
    "    #Pad sequences with 0s to make sure that the length is the same:\n",
    "\n",
    "    print('Pad sequences (samples x time)')\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=feature_len)\n",
    "    X_dev = sequence.pad_sequences(X_dev, maxlen=feature_len)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=feature_len)\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('X_dev shape:', X_dev.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "\n",
    "    num_classes = np.max(y_train) + 1\n",
    "    print(num_classes, 'classes')\n",
    "\n",
    "    print('Convert class vector to binary class matrix '\n",
    "          '(for use with categorical_crossentropy)')\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_dev = keras.utils.to_categorical(y_dev, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('y_dev shape:', y_dev.shape)\n",
    "    print('y_test shape:', y_test.shape)\n",
    "    print('Number of classes:', y_train.shape[1])\n",
    "    \n",
    "    #Getting zipcode composition data for train, test, dev:\n",
    "    \n",
    "    if set([\"frac_white\",\"frac_black\",\"frac_asian\"]).issubset(sdf.columns):\n",
    "        \n",
    "        sub=sdf[[\"frac_white\",\"frac_black\",\"frac_asian\"]]\n",
    "    \n",
    "        N_test=sub[:ntest]\n",
    "    \n",
    "        N_dev=sub[ntest:ndev]\n",
    "    \n",
    "        N_train=sub[ndev:]\n",
    "\n",
    "    return vocab, X_train, X_dev, X_test, y_train, y_dev, y_test, N_test, N_dev, N_train\n",
    "\n",
    "\n",
    "def build_model_lstm_zipcode_composition(num_words,num_classes,feature_len,output_length,num_length):\n",
    "    nlp_input = Input(shape=(feature_len,)) \n",
    "    meta_input = Input(shape=(num_length,))\n",
    "    emb = Embedding(num_words, output_length, input_length=feature_len)(nlp_input) \n",
    "    nlp_out = LSTM(128)(emb) \n",
    "    concat = concatenate([nlp_out, meta_input]) \n",
    "    classifier = Dense(32, activation='relu')(concat) \n",
    "    output = Dense(num_classes, activation='sigmoid')(classifier) \n",
    "    model = Model(inputs=[nlp_input , meta_input], outputs=[output])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_model_lstm(num_words,num_classes,feature_len,output_length):\n",
    "    print('Build model...')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, output_length, input_length=feature_len))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_model_conv_lstm(num_words,num_classes,feature_len,output_length):\n",
    "    model= Sequential()\n",
    "    model.add(Embedding(num_words, output_length, input_length=feature_len))\n",
    "    model.add(Conv1D(activation=\"relu\", padding=\"same\", filters=32, kernel_size=3))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def model_evaluate(model,X,y):\n",
    "    y_pred = model.predict_classes(X, verbose=2)\n",
    "    \n",
    "    p = model.predict_proba(X, verbose=2) # to predict probability\n",
    "    \n",
    "    target_names = list(df.race.astype('category').cat.categories)\n",
    "    \n",
    "    print(\"Model's performance:\")\n",
    "    print(classification_report(np.argmax(y, axis=1), y_pred, target_names=target_names))\n",
    "    print(confusion_matrix(np.argmax(y, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2bdf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the data:\n",
    "df = pd.read_csv('/Volumes/Samsung_T5/nameracesample.csv',dtype={'name_last':str,'name_first':str,'fips':str,'zip':str,'race':str,\n",
    "                                                                'frac_white':float,'frac_black':float,'frac_asian':float})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1837839",
   "metadata": {},
   "source": [
    "Training the baseline model using data on first and last names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523319be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "sample=2000000\n",
    "ntest=50000\n",
    "ngrams=(2,2) #Range of n-grams\n",
    "feature_len=25 #Sequence length:\n",
    "mymodel='fl'\n",
    "\n",
    "vocab, X_train, X_dev, X_test, y_train, y_dev, y_test, _, _, _=data_preprocessing(df,sample,ntest,ngrams,feature_len,model=mymodel,random_state=21)\n",
    "\n",
    "num_classes=y_train.shape[1]\n",
    "num_words=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3391e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "feature_len=25 #Sequence length\n",
    "output_length=32 #Length of embedding\n",
    "batch_size=512 \n",
    "epochs=7\n",
    "model_ouput_name=\"emb_lstm128_drop02_fl\"\n",
    "\n",
    "model_lstm = build_model_lstm(num_words,num_classes,feature_len,output_length)\n",
    "\n",
    "history=model_lstm.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(X_dev, y_dev),verbose=1)\n",
    "\n",
    "#Plot accuracy and loss:\n",
    "plot_loss_accuracy(history)\n",
    "\n",
    "#Evaluate the model on the dev and test sets:\n",
    "model_evaluate(model_lstm,X_dev,y_dev)\n",
    "model_evaluate(model_lstm,X_test,y_test)\n",
    "\n",
    "model_lstm.save('/Users/nkotova/Documents/CS 230 Deep Learning/project/models/'+model_ouput_name+\".tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64179ccb",
   "metadata": {},
   "source": [
    "Training the alternative architecture (CONV+MaxPool+LSTM) using first and last name data.\n",
    "Performs slightly worse than LSTM, but runs much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "feature_len=25 #Sequence length\n",
    "output_length=32 #Embedding\n",
    "batch_size=512\n",
    "epochs=7\n",
    "model_name=\"emb_conv_maxpool_lstm100_drop02_l\"\n",
    "\n",
    "\n",
    "model_conv_lstm= build_model_conv_lstm(num_words,num_classes,feature_len,output_length)\n",
    "\n",
    "history_conv_lstm=model_conv_lstm.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(X_dev, y_dev),verbose=1)\n",
    "\n",
    "plot_loss_accuracy(history_conv_lstm)\n",
    "\n",
    "model_evaluate(model_conv_lstm,X_dev,y_dev)\n",
    "model_evaluate(model_conv_lstm,X_test,y_test)\n",
    "\n",
    "model_conv_lstm.save('/Users/nkotova/Documents/CS 230 Deep Learning/project/models/'+model_name+'.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740cbef",
   "metadata": {},
   "source": [
    "Sampling fully balanced training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc426e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "sample=500000\n",
    "ntest=50000\n",
    "ngrams=(2,2) #Range of n-grams\n",
    "feature_len=25 #Sequence length:\n",
    "mymodel='fl'\n",
    "strat_sampling=True\n",
    "\n",
    "vocab, X_train, X_dev, X_test, y_train, y_dev, y_test, _, _, _=data_preprocessing(df,sample,ntest,ngrams,feature_len,model=mymodel,strat_sampling=strat_sampling,random_state=21)\n",
    "\n",
    "num_classes=y_train.shape[1]\n",
    "num_words=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf650fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "feature_len=25 #Sequence length\n",
    "output_length=32 #Embedding\n",
    "batch_size=512\n",
    "epochs=7\n",
    "model_name=\"strat_emb_lstm128_drop02_l\"\n",
    "\n",
    "model_lstm_bal= build_model_lstm(num_words,num_classes,feature_len,output_length)\n",
    "\n",
    "history_lstm_bal=model_lstm_bal.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(X_dev, y_dev),verbose=1)\n",
    "\n",
    "plot_loss_accuracy(history_lstm_bal)\n",
    "\n",
    "model_evaluate(model_lstm_bal,X_dev,y_dev)\n",
    "model_evaluate(model_lstm_bal,X_test,y_test)\n",
    "\n",
    "model_lstm_bal.save('/Users/nkotova/Documents/CS 230 Deep Learning/project/models/'+model_name+'.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967019f8",
   "metadata": {},
   "source": [
    "Sampling unbalanced dataset with zipcode strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "sample=2000000\n",
    "ntest=50000\n",
    "ngrams=(2,2) #Range of n-grams\n",
    "feature_len=25 #Sequence length:\n",
    "mymodel='fl_zip'\n",
    "\n",
    "vocab, X_train, X_dev, X_test, y_train, y_dev, y_test, _, _, _=data_preprocessing(df,sample,ntest,ngrams,feature_len,model=mymodel,random_state=21)\n",
    "\n",
    "num_classes=y_train.shape[1]\n",
    "num_words=len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484c30c",
   "metadata": {},
   "source": [
    "Training baseline model on last name, first name and zipcode string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "output_length=32 #Embedding\n",
    "batch_size=512\n",
    "epochs=7\n",
    "model_name=\"emb_lstm128_drop02_fl_zip\"\n",
    "\n",
    "model_lstm_zip=build_model_lstm(num_words,num_classes,feature_len,output_length)\n",
    "\n",
    "history_lstm_zip=model_lstm_zip.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(X_dev, y_dev),verbose=1)\n",
    "\n",
    "plot_loss_accuracy(history)\n",
    "\n",
    "model_evaluate(model_lstm_zip,X_dev,y_dev)\n",
    "model_evaluate(model_lstm_zip,X_test,y_test)\n",
    "\n",
    "model_lstm_zip.save('/Users/nkotova/Documents/CS 230 Deep Learning/project/models/'+model_name+'.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d106c52",
   "metadata": {},
   "source": [
    "Sampling data for the baseline LSTM model with additional data on zipcode composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5de2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling data for LSTM model with additional data on zipcode composition:\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "\n",
    "#Parameters:\n",
    "sample=2000000\n",
    "ntest=50000\n",
    "ngrams=(2,2) #Range of n-grams\n",
    "feature_len=25 #Sequence length:\n",
    "mymodel='fl_zipcomp'\n",
    "\n",
    "vocab, X_train, X_dev, X_test, y_train, y_dev, y_test, N_test, N_dev, N_train=data_preprocessing(df,sample,ntest,ngrams,feature_len,model=mymodel,random_state=21)\n",
    "\n",
    "num_classes=y_train.shape[1]\n",
    "num_words=len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8b113",
   "metadata": {},
   "source": [
    "Training LSTM model with additional data on zipcode composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "output_length=32 #Embedding\n",
    "batch_size=512\n",
    "epochs=7\n",
    "model_name=\"emb_lstm128_numdense_fl_zipcomp\"\n",
    "\n",
    "model_lstm_num=build_model_lstm_num(num_words,num_classes,feature_len,output_length,num_length)\n",
    "\n",
    "history=model_lstm_num.fit([X_train, N_train], y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=([X_dev, N_dev], y_dev),verbose=1)\n",
    "\n",
    "plot_loss_accuracy(history)\n",
    "\n",
    "#Evaluating the model:\n",
    "\n",
    "y_pred = model_lstm_num.predict([X_dev, N_dev], verbose=2)\n",
    "y_pred=np.argmax(y_pred,axis=1)\n",
    "target_names = list(df.race.astype('category').cat.categories)\n",
    "print(\"Performance on dev set\")\n",
    "print(classification_report(np.argmax(y_dev,axis=1), y_pred, target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_dev, axis=1), y_pred))\n",
    "\n",
    "y_pred = model_lstm_num.predict([X_test, N_test], verbose=2)\n",
    "y_pred=np.argmax(y_pred,axis=1)\n",
    "target_names = list(df.race.astype('category').cat.categories)\n",
    "print(\"Performance on test set\")\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred, target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "model_lstm_num.save('models/'+model_name+'.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977bcb5",
   "metadata": {},
   "source": [
    "Sampling data for last name only model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "sample=2000000\n",
    "ntest=50000\n",
    "ngrams=(2,2) #Range of n-grams\n",
    "feature_len=25 #Sequence length:\n",
    "mymodel='l'\n",
    "\n",
    "\n",
    "vocab, X_train, X_dev, X_test, y_train, y_dev, y_test, _, _, _=data_preprocessing(df,sample,ntest,ngrams,feature_len,model=mymodel,random_state=21)\n",
    "\n",
    "num_classes=y_train.shape[1]\n",
    "num_words=len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d18bcf",
   "metadata": {},
   "source": [
    "Training last name only data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "output_length=32 #Embedding\n",
    "batch_size=512\n",
    "epochs=7\n",
    "model_name=\"emb_lstm128_drop02_l\"\n",
    "\n",
    "model_lstm_ln= build_model_lstm(num_words,num_classes,feature_len,output_length)\n",
    "\n",
    "history_lstm_ln=model_lstm_ln.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(X_dev, y_dev),verbose=1)\n",
    "\n",
    "plot_loss_accuracy(history_lstm_ln)\n",
    "\n",
    "model_evaluate(model_lstm_ln,X_dev,y_dev)\n",
    "model_evaluate(model_lstm_ln,X_test,y_test)\n",
    "\n",
    "model_lstm_ln.save('/Users/nkotova/Documents/CS 230 Deep Learning/project/models/'+model_name+'.tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
